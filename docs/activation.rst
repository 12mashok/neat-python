
Overview of builtin activation functions
========================================

The implementation of these functions can be found in the `nn module
<https://github.com/CodeReclaimers/neat-python/blob/master/neat/nn/__init__.py>`_.

abs
---

.. figure:: activation-abs.png
   :scale: 50 %
   :alt: absolute value function


clamped
-------

.. figure:: activation-clamped.png
   :scale: 50 %
   :alt: clamped linear function

cube
----

.. figure:: activation-cube.png
   :scale: 50 %
   :alt: cubic function

exp
---

.. figure:: activation-exp.png
   :scale: 50 %
   :alt: exponential function


gauss
-----

.. figure:: activation-gauss.png
   :scale: 50 %
   :alt: gaussian function

hat
---

.. figure:: activation-hat.png
   :scale: 50 %
   :alt: hat function

identity
--------

.. figure:: activation-identity.png
   :scale: 50 %
   :alt: identity function

inv
---

.. figure:: activation-inv.png
   :scale: 50 %
   :alt: inverse function

log
---

.. figure:: activation-log.png
   :scale: 50 %
   :alt: log function

relu
----

.. figure:: activation-relu.png
   :scale: 50 %
   :alt: rectified linear function

sigmoid
-------

.. figure:: activation-sigmoid.png
   :scale: 50 %
   :alt: sigmoid function

sin
---

.. figure:: activation-sin.png
   :scale: 50 %
   :alt: sine function

softplus
--------

.. figure:: activation-softplus.png
   :scale: 50 %
   :alt: soft-plus function

square
------

.. figure:: activation-square.png
   :scale: 50 %
   :alt: square function

tanh
----

.. figure:: activation-tanh.png
   :scale: 50 %
   :alt: hyperbolic tangent function























